"""
Enhanced Asynchronous Request Manager with Celery Integration
Provides comprehensive throttling protection and rate limiting for AWS Bedrock API calls

Features:
1. Asynchronous task queue using Celery
2. Intelligent rate limiting (prevents AWS 503 errors)
3. Multi-model fallback with exponential backoff
4. Token-aware request batching
5. Per-user fair scheduling
6. Comprehensive error recovery
"""

import os
import time
import uuid
from datetime import datetime, timedelta
from collections import defaultdict, deque
from typing import Dict, List, Optional, Tuple, Any
import threading
import json

# Note: Redis removed - using in-memory rate limiting instead
# Rate limiting state is per-process, which works fine for our use case

# Rate Limiting Configuration
class RateLimitConfig:
    """
    AWS Bedrock Rate Limits (conservative settings to prevent throttling)

    Based on AWS Bedrock standard limits:
    - Invoke Model: 100 requests per minute per region
    - Token throughput: 200,000 tokens per minute per region

    We use conservative limits (60-70% of max) to ensure stability
    """
    # Request rate limits
    MAX_REQUESTS_PER_MINUTE = 30  # Conservative: 30% of AWS limit
    MAX_CONCURRENT_REQUESTS = 5    # Max concurrent API calls

    # Token rate limits
    MAX_TOKENS_PER_MINUTE = 120000  # 60% of AWS limit
    MAX_TOKENS_PER_REQUEST = 8192    # Claude 3.5 Sonnet default

    # Cooldown periods
    THROTTLE_COOLDOWN_SECONDS = 60   # Wait 60s after throttling
    MODEL_SWITCH_DELAY_SECONDS = 5   # Delay before trying next model

    # Retry configuration
    MAX_RETRIES_PER_REQUEST = 3      # Max retries per request
    INITIAL_BACKOFF_SECONDS = 2      # Initial exponential backoff
    MAX_BACKOFF_SECONDS = 120        # Maximum backoff time

    # Monitoring
    ERROR_THRESHOLD_FOR_CIRCUIT_BREAKER = 5  # Open circuit after 5 consecutive errors


class TokenCounter:
    """
    Token counter for request rate limiting
    Uses approximate token counts to prevent exceeding AWS token limits
    """

    def __init__(self):
        self.tokens_per_minute = deque()
        self.lock = threading.Lock()

    def estimate_tokens(self, text: str) -> int:
        """
        Estimate token count for text
        Claude uses ~4 characters per token on average
        """
        if not text:
            return 0
        return len(text) // 4 + 100  # Add buffer for formatting tokens

    def add_request(self, prompt_tokens: int, completion_tokens: int = 0):
        """Record token usage for a request"""
        with self.lock:
            now = datetime.now()
            total_tokens = prompt_tokens + completion_tokens
            self.tokens_per_minute.append((now, total_tokens))

            # Clean up old entries (older than 1 minute)
            cutoff_time = now - timedelta(minutes=1)
            while self.tokens_per_minute and self.tokens_per_minute[0][0] < cutoff_time:
                self.tokens_per_minute.popleft()

    def get_tokens_last_minute(self) -> int:
        """Get total tokens used in the last minute"""
        with self.lock:
            now = datetime.now()
            cutoff_time = now - timedelta(minutes=1)

            # Clean up old entries
            while self.tokens_per_minute and self.tokens_per_minute[0][0] < cutoff_time:
                self.tokens_per_minute.popleft()

            # Sum remaining tokens
            return sum(tokens for _, tokens in self.tokens_per_minute)

    def can_make_request(self, estimated_tokens: int) -> Tuple[bool, Optional[float]]:
        """
        Check if request can be made without exceeding token limit

        Returns:
            (can_make_request, wait_seconds)
        """
        current_usage = self.get_tokens_last_minute()

        if current_usage + estimated_tokens <= RateLimitConfig.MAX_TOKENS_PER_MINUTE:
            return True, None

        # Calculate wait time (wait until oldest tokens expire)
        if self.tokens_per_minute:
            oldest_time = self.tokens_per_minute[0][0]
            wait_seconds = 60 - (datetime.now() - oldest_time).total_seconds()
            return False, max(0, wait_seconds)

        return False, 60  # Default wait


class AsyncRequestManager:
    """
    Enhanced asynchronous request manager with comprehensive throttling protection

    Architecture:
    1. Redis-backed distributed rate limiting
    2. Celery task queue for async execution
    3. Multi-model fallback with health tracking
    4. Token-aware request scheduling
    5. Circuit breaker pattern for error recovery
    """

    def __init__(self, redis_url: Optional[str] = None):
        """
        Initialize the async request manager

        Note: Redis has been removed - using in-memory rate limiting
        """
        # Redis removed - using in-memory rate limiting (works fine for single-region deployment)
        self.redis_available = False
        self.redis_client = None
        print("âœ… Using in-memory rate limiting (SQS-based deployment)")

        # Rate limiting state
        self.request_timestamps = deque()
        self.token_counter = TokenCounter()
        self.active_requests = 0
        self.lock = threading.Lock()

        # Model health tracking
        self.model_health = defaultdict(lambda: {
            'status': 'healthy',
            'consecutive_errors': 0,
            'last_error_time': None,
            'cooldown_until': None,
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0
        })

        # Statistics
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'throttled_requests': 0,
            'retried_requests': 0,
            'fallback_used': 0,
            'circuit_breaker_trips': 0,
            'avg_response_time': 0.0
        }
        self.stats_lock = threading.Lock()

        print(f"âœ… AsyncRequestManager initialized")
        print(f"   Max requests/min: {RateLimitConfig.MAX_REQUESTS_PER_MINUTE}")
        print(f"   Max concurrent: {RateLimitConfig.MAX_CONCURRENT_REQUESTS}")
        print(f"   Max tokens/min: {RateLimitConfig.MAX_TOKENS_PER_MINUTE}")

    def can_make_request(self) -> Tuple[bool, Optional[str]]:
        """
        Check if a new request can be made based on rate limits

        Returns:
            (can_make, reason)
        """
        with self.lock:
            now = datetime.now()

            # Check concurrent request limit
            if self.active_requests >= RateLimitConfig.MAX_CONCURRENT_REQUESTS:
                return False, f"Max concurrent requests ({RateLimitConfig.MAX_CONCURRENT_REQUESTS}) reached"

            # Clean up old timestamps (> 1 minute old)
            cutoff_time = now - timedelta(minutes=1)
            while self.request_timestamps and self.request_timestamps[0] < cutoff_time:
                self.request_timestamps.popleft()

            # Check requests per minute limit
            if len(self.request_timestamps) >= RateLimitConfig.MAX_REQUESTS_PER_MINUTE:
                wait_time = 60 - (now - self.request_timestamps[0]).total_seconds()
                return False, f"Rate limit reached, wait {wait_time:.1f}s"

            return True, None

    def wait_for_rate_limit(self, estimated_tokens: int = 0) -> float:
        """
        Wait until rate limits allow the request

        Args:
            estimated_tokens: Estimated token count for the request

        Returns:
            wait_seconds: Time waited (for logging)
        """
        start_time = time.time()

        while True:
            # Check request rate limit
            can_make, reason = self.can_make_request()

            if not can_make:
                print(f"â¸ï¸ Rate limit: {reason}")
                time.sleep(1)
                continue

            # Check token rate limit
            if estimated_tokens > 0:
                can_make_tokens, wait_seconds = self.token_counter.can_make_request(estimated_tokens)

                if not can_make_tokens:
                    print(f"â¸ï¸ Token limit: wait {wait_seconds:.1f}s")
                    time.sleep(min(wait_seconds, 5))  # Sleep in chunks
                    continue

            # Both checks passed
            break

        wait_time = time.time() - start_time
        if wait_time > 1:
            print(f"â³ Waited {wait_time:.1f}s for rate limit clearance")

        return wait_time

    def record_request_start(self):
        """Record that a request is starting"""
        with self.lock:
            self.request_timestamps.append(datetime.now())
            self.active_requests += 1

    def record_request_end(self, success: bool, model_id: str, duration: float,
                          tokens_used: int = 0, error: Optional[str] = None):
        """
        Record request completion

        Args:
            success: Whether request succeeded
            model_id: Model that was used
            duration: Request duration in seconds
            tokens_used: Tokens consumed
            error: Error message if failed
        """
        with self.lock:
            self.active_requests -= 1

        # Update statistics
        with self.stats_lock:
            self.stats['total_requests'] += 1

            if success:
                self.stats['successful_requests'] += 1
                # Update rolling average response time
                n = self.stats['successful_requests']
                old_avg = self.stats['avg_response_time']
                self.stats['avg_response_time'] = (old_avg * (n - 1) + duration) / n
            else:
                self.stats['failed_requests'] += 1

                # Check if throttling error
                if error and ('throttl' in error.lower() or 'too many' in error.lower() or '503' in error):
                    self.stats['throttled_requests'] += 1

        # Update model health
        model_health = self.model_health[model_id]
        model_health['total_requests'] += 1

        if success:
            model_health['successful_requests'] += 1
            model_health['consecutive_errors'] = 0
            model_health['status'] = 'healthy'
        else:
            model_health['failed_requests'] += 1
            model_health['consecutive_errors'] += 1
            model_health['last_error_time'] = datetime.now()

            # Check if circuit breaker should trip
            if model_health['consecutive_errors'] >= RateLimitConfig.ERROR_THRESHOLD_FOR_CIRCUIT_BREAKER:
                model_health['status'] = 'circuit_open'
                cooldown_seconds = RateLimitConfig.THROTTLE_COOLDOWN_SECONDS * model_health['consecutive_errors']
                model_health['cooldown_until'] = datetime.now() + timedelta(seconds=cooldown_seconds)

                with self.stats_lock:
                    self.stats['circuit_breaker_trips'] += 1

                print(f"ðŸš« Circuit breaker opened for {model_id}, cooldown: {cooldown_seconds}s")

        # Record token usage
        if tokens_used > 0:
            self.token_counter.add_request(tokens_used)

    def is_model_available(self, model_id: str) -> Tuple[bool, Optional[str]]:
        """
        Check if a model is available for use

        Returns:
            (is_available, reason)
        """
        health = self.model_health[model_id]

        # Check circuit breaker
        if health['status'] == 'circuit_open':
            if health['cooldown_until'] and datetime.now() < health['cooldown_until']:
                remaining = (health['cooldown_until'] - datetime.now()).total_seconds()
                return False, f"Circuit breaker open, cooldown: {remaining:.0f}s"
            else:
                # Cooldown expired, reset to half-open
                health['status'] = 'half_open'
                health['consecutive_errors'] = 0
                print(f"âœ… Circuit breaker half-open for {model_id}, trying again")

        return True, None

    def get_stats(self) -> Dict[str, Any]:
        """Get current statistics"""
        with self.stats_lock:
            stats = self.stats.copy()

        stats['active_requests'] = self.active_requests
        stats['requests_last_minute'] = len(self.request_timestamps)
        stats['tokens_last_minute'] = self.token_counter.get_tokens_last_minute()
        stats['model_health'] = {}

        for model_id, health in self.model_health.items():
            stats['model_health'][model_id] = health.copy()

        return stats

    def reset_model_health(self, model_id: Optional[str] = None):
        """
        Reset model health tracking (emergency recovery)

        Args:
            model_id: Specific model to reset, or None for all models
        """
        if model_id:
            self.model_health[model_id] = {
                'status': 'healthy',
                'consecutive_errors': 0,
                'last_error_time': None,
                'cooldown_until': None,
                'total_requests': 0,
                'successful_requests': 0,
                'failed_requests': 0
            }
            print(f"ðŸ”„ Reset health for {model_id}")
        else:
            self.model_health.clear()
            print("ðŸ”„ Reset health for all models")


# Global instance
_async_request_manager = None
_manager_lock = threading.Lock()

def get_async_request_manager() -> AsyncRequestManager:
    """Get or create the global async request manager instance"""
    global _async_request_manager

    with _manager_lock:
        if _async_request_manager is None:
            _async_request_manager = AsyncRequestManager()

        return _async_request_manager
